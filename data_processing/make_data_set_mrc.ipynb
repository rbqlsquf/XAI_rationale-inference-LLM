{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "def create_example(all_data):\n",
                "    all_result = []\n",
                "    for data in tqdm(all_data):\n",
                "        data_id = data[\"_id\"]\n",
                "        Question = data[\"question\"]\n",
                "        answer = data[\"answer\"]\n",
                "        context = data[\"context\"]\n",
                "        supporting_facts = data[\"supporting_facts\"]\n",
                "        support_dic = {}\n",
                "        for sup_sent in supporting_facts:\n",
                "            title = sup_sent[0]  # supporting fact의 제목\n",
                "            set_num = sup_sent[1]  # 문장번호\n",
                "            if title not in support_dic.keys():\n",
                "                support_dic[title] = []\n",
                "            support_dic[title].append(set_num)\n",
                "        \n",
                "        supporting_num_list = []\n",
                "        sent_list = []\n",
                "        sentences = \"\"\n",
                "        sent_num = 1\n",
                "        for index, j in enumerate(context):\n",
                "            title = j[0]\n",
                "            sent_list.append(title.strip()+ \".\")\n",
                "            sent_num += 1\n",
                "            \n",
                "            if title in support_dic.keys():\n",
                "                for i in support_dic[title]:\n",
                "                    supporting_num_list.append(sent_num+ i )\n",
                "    \n",
                "            if sentences == \"\":\n",
                "                sentences = title + \". \"\n",
                "            else:\n",
                "                sentences = sentences + \" \" + title + \". \"\n",
                "            for sent in j[1]:\n",
                "                sentences = sentences + sent\n",
                "                sent_list.append(sent.strip())\n",
                "                sent_num += 1\n",
                "            \n",
                "            assert (sent_num -1) == len(sent_list)\n",
                "            \n",
                "        result = {}\n",
                "        result[\"_id\"] = data_id\n",
                "        result[\"question\"] = Question\n",
                "        result[\"document\"] = sentences\n",
                "        result[\"sent\"] = sent_list\n",
                "        result[\"supporting_num\"] = supporting_num_list\n",
                "        result[\"output\"] = answer\n",
                "        \n",
                "\n",
                "        all_result.append(result)\n",
                "\n",
                "    return all_result\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 7405/7405 [00:00<00:00, 35359.17it/s]\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "# file_path = \"../data/origin/hotpot_train_v1.1_re.json\"\n",
                "file_path = \"../data/origin/hotpot_dev.json\"\n",
                "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
                "    dev_data = json.load(file)\n",
                "\n",
                "input_data = create_example(dev_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "7405"
                        ]
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(input_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 7405/7405 [00:33<00:00, 221.98it/s]\n"
                    ]
                }
            ],
            "source": [
                "from tqdm import tqdm\n",
                "all_len = []\n",
                "all_result = []\n",
                "\n",
                "for example in tqdm(input_data):\n",
                "    MAX_LENGTH = 2048\n",
                "    input_ids, attention_mask, labels = [], [], []\n",
                "    example[\"document\"] = example[\"document\"].strip()\n",
                "    # token 된 doc\n",
                "    token_doc = {\"input_ids\": [], \"attention_mask\": []}\n",
                "    # document 문장 index\n",
                "    sentence_number = 0\n",
                "    sentence_position = []\n",
                "    for i, sent in enumerate(example[\"sent\"]):\n",
                "        # 0번 문장은 instruction으로 지정할 계획\n",
                "        sent = sent.strip()\n",
                "        token_sent = tokenizer(sent + \" \", add_special_tokens=False)\n",
                "        sentence_number += 1  # 1부터 시작\n",
                "        sentence_position.extend([sentence_number] * len(token_sent[\"input_ids\"]))\n",
                "        token_doc[\"input_ids\"] += token_sent[\"input_ids\"]\n",
                "        token_doc[\"attention_mask\"] += token_sent[\"attention_mask\"]\n",
                "    token_end = tokenizer(\"<|im_end|>\\n\", add_special_tokens=False)\n",
                "    sentence_position.extend([0] * len(token_end))\n",
                "    token_doc[\"input_ids\"] += token_end[\"input_ids\"]\n",
                "    token_doc[\"attention_mask\"] += token_end[\"attention_mask\"]\n",
                "    instruction = tokenizer(\n",
                "        f\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n**Question:{example['question']}\\n**Document:\\n\",\n",
                "        add_special_tokens=False,\n",
                "    )\n",
                "    response = tokenizer(\n",
                "        f\"<|im_start|>assistant\\n**Answer:{example['output'].strip()}<|im_end|>\\n\", add_special_tokens=False\n",
                "    )\n",
                "    \n",
                "    input_ids = instruction[\"input_ids\"] + token_doc[\"input_ids\"] + response[\"input_ids\"]\n",
                "    count = len(input_ids)\n",
                "    if count <= MAX_LENGTH:\n",
                "        all_result.append(example)\n",
                "    all_len.append(count)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "6913\n",
                        "1495.5786630654964\n"
                    ]
                }
            ],
            "source": [
                "print(len(all_result))\n",
                "print(sum(all_len)/len(all_len))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<|im_start|>system\n",
                        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "**Question:Blackfin is a family of processors developed by the company that is headquartered in what city?\n",
                        "**Document:\n",
                        "1st Word/1st Word Plus. 1st Word and 1st Word Plus are word processors developed by GST Computer Systems in the 1980s. The original package, 1st Word, was given away free with all Atari STs. The later 1st Word Plus was sold by GST and was more advanced. Atari ST disk magazine ST News was written entirely and exclusively using 1st Word and, later, 1st Word Plus. The first Volume (1986) was distributed as a plain 1st Word . DOC file, after that a custom shell was produced that enabled the 1st Word documents to be displayed in a userfriendly disk magazine shell. Arm Holdings. Arm Holdings (Arm) is a British multinational semiconductor and software design company, owned by SoftBank Group and its Vision Fund. Headquartered in Cambridge, United Kingdom, its primary business is in the design of Arm processors (CPUs), although it also designs software development tools under the DS-5, RealView and Keil brands, as well as systems and platforms, system-on-a-chip (SoC) infrastructure and software. It is considered to be market dominant for processors in mobile phones (smartphones or otherwise) and tablet computers. The company is one of the best-known 'Silicon Fen' companies. PowerPC e200. The e200 core is developed from the MPC5xx family processors, which in turn is derived from the MPC8xx core in the PowerQUICC SoC processors. e200 adheres to the Power ISA v.2.03 as well as the previous \"Book E\" specification. All e200 core based microprocessors are named in the MPC55xx and MPC56xx/JPC56x scheme, not to be confused with the MPC52xx processors which is based on the PowerPC e300 core. OMAP. OMAP (Open Multimedia Applications Platform) is a series of image/video processors developed by Texas Instruments. They are a category of proprietary system on chips (SoCs) for portable and mobile multimedia applications. OMAP devices generally include a general-purpose ARM architecture processor core plus one or more specialized co-processors. Earlier OMAP variants commonly featured a variant of the Texas Instruments TMS320 series digital signal processor. Analog Devices. Analog Devices, Inc., also known as ADI or Analog, is an American multinational semiconductor company specializing in data conversion and signal processing technology, headquartered in Norwood, Massachusetts. In 2012, Analog Devices led the worldwide data converter market with a 48.5% share, according to analyst firm Databeans. Intel. Intel Corporation (also known as Intel, stylized as intel) is an American multinational corporation and technology company headquartered in Santa Clara, California (colloquially referred to as \"Silicon Valley\") that was founded by Gordon Moore (of Moore's law fame) and Robert Noyce. It is the world's second largest and second highest valued semiconductor chip makers based on revenue after being overtaken by Samsung, and is the inventor of the x86 series of microprocessors, the processors found in most personal computers (PCs). Intel supplies processors for computer system manufacturers such as Apple, Lenovo, HP, and Dell. Intel also manufactures motherboard chipsets, network interface controllers and integrated circuits, flash memory, graphics chips, embedded processors and other devices related to communications and computing. Zet (hardware). Zet is a clone x86 processor where its machine code compatible with x86 processors developed as an effort to make open-hardware processor. Xetal. Xetal is the name of a family of non commercial massively parallel processors developed within Philips Research. . XAP processor. The XAP processor is a RISC processor architecture developed by Cambridge Consultants since 1994. XAP processors are a family of 16-bit and 32-bit cores, all of which are intended for use in an application-specific integrated circuit or ASIC chip design. XAP processors were designed for use in mixed-signal integrated circuits for sensor or wireless applications including Bluetooth, ZigBee, GPS, RFID or Near Field Communication chips. Typically these integrated circuits are used in low cost, high volume products that are battery-powered and must have low energy consumption. There are other applications where XAP processors have been used to good effect, such as wireless sensor networks and medical devices, e.g. hearing aids. Blackfin. The Blackfin is a family of 16- or 32-bit microprocessors developed, manufactured and marketed by Analog Devices. The processors have built-in, fixed-point digital signal processor (DSP) functionality supplied by 16-bit Multiply–accumulates (MACs), accompanied on-chip by a small microcontroller. It was designed for a unified low-power processor architecture that can run operating systems while simultaneously handling complex numeric tasks such as real-time H.264 video encoding. There are several hardware development kits for the Blackfin. Open-source operating systems for the Blackfin include uClinux. <|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "**Answer:Norwood, Massachusetts<|im_end|>\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(tokenizer.decode(input_ids))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path = \"../data/1113data/hotpot_dev.json\"\n",
                "with open(file_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(all_result, f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path = \"../data/1113data/hotpot_train.json\"\n",
                "with open(file_path, 'r', encoding='utf-8') as f:\n",
                "    data = json.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "random.seed(42)\n",
                "random.shuffle(data)\n",
                "\n",
                "write_path = \"../data/1113data/hotpot_train_shuffle.json\"\n",
                "with open(write_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(data[:30000], f, ensure_ascii=False, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
