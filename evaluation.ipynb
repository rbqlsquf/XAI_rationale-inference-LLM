{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_answer(s):\n",
    "    \"\"\"간단한 토큰화와 정규화\"\"\"\n",
    "    s = s.lower()  # 소문자 변환\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)  # 불필요한 관사 제거\n",
    "    s = re.sub(r'[^a-z0-9]', ' ', s)  # 알파벳과 숫자 외 제거\n",
    "    return ' '.join(s.split())  # 공백 정리\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"예측 답과 실제 답 간의 EM 점수 계산\"\"\"\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score_hotpot(prediction, ground_truth):\n",
    "    \"\"\"예측 답과 실제 답 간의 F1 점수 계산\"\"\"\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gt_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(gt_tokens)\n",
    "    num_common = len(common_tokens)\n",
    "    \n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(gt_tokens)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/1103+dataup+loss/hotpot_tt_8000.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dev_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "----\n",
      "Chief of Protocol\n",
      "United States ambassador\n",
      "----\n",
      "Animorphs\n",
      "Animorphs\n",
      "----\n",
      "no\n",
      "no\n",
      "----\n",
      "Greenwich Village, New York City\n",
      "Greenwich Village\n",
      "----\n",
      "YG Entertainment\n",
      "YG Entertainment\n",
      "----\n",
      "Eenasul Fateh\n",
      "James P. Comer\n",
      "----\n",
      "3,677 seated\n",
      "4,000\n",
      "----\n",
      "Terry Richardson\n",
      "Terry Richardson\n",
      "----\n",
      "yes\n",
      "yes\n",
      "----\n",
      "Kansas Song\n",
      "Kansas Song\n",
      "----\n",
      "David Weissman\n",
      "David Weissman\n",
      "----\n",
      "1999\n",
      "1994\n",
      "----\n",
      "no\n",
      "\n",
      "----\n",
      "from 1986 to 2013\n",
      "1986 to 2013\n",
      "----\n",
      "9,984\n",
      "9,984\n",
      "----\n",
      "the North Atlantic Conference\n",
      "Eastern College Athletic Conference-North\n",
      "----\n",
      "yes\n",
      "yes\n",
      "----\n",
      "1969 until 1974\n",
      "1969–1974\n",
      "----\n",
      "Robert Erskine Childers DSC\n",
      "Robert Erskine Childers\n",
      "----\n",
      "Pedro Rodríguez\n",
      "Sergio Pérez\n",
      "----\n",
      "Sonic\n",
      "Tigger\n",
      "----\n",
      "keyboard function keys\n",
      "Front Row\n",
      "----\n",
      "Badly Drawn Boy\n",
      "Wolf Alice\n",
      "----\n",
      "World's Best Goalkeeper\n",
      "Peter Schmeichel\n",
      "----\n",
      "Barton Lee Hazlewood\n",
      "Lee Hazlewood\n",
      "----\n",
      "1838\n",
      "1838\n",
      "----\n",
      "yes\n",
      "yes\n",
      "----\n",
      "Henry J. Kaiser\n",
      "Henry J. Kaiser\n",
      "----\n",
      "Arena of Khazan\n",
      "Crusaders of Khazan\n",
      "----\n",
      "2000\n",
      "2000\n",
      "----\n",
      "Fujioka, Gunma\n",
      "Japan\n",
      "----\n",
      "Charles Eugène\n",
      "Charles Nungesser\n",
      "----\n",
      "no\n",
      "no\n",
      "----\n",
      "Letters to Cleo\n",
      "Screaming Trees\n",
      "----\n",
      "October 1922\n",
      "1922\n",
      "----\n",
      "2000\n",
      "2000\n",
      "----\n",
      "World War II\n",
      "World War II\n",
      "----\n",
      "no\n",
      "no\n",
      "----\n",
      "New York City\n",
      "New York City\n",
      "----\n",
      "Scotch Collie\n",
      "Scotch Collie\n",
      "----\n",
      "Mumbai\n",
      "Mumbai, Maharashtra\n",
      "----\n",
      "1962\n",
      "1962\n",
      "----\n",
      "sovereignty\n",
      "sovereignty\n",
      "----\n",
      "Nelson Rockefeller\n",
      "Nelson Rockefeller\n",
      "----\n",
      "Yellowcraig\n",
      "Firth of Forth\n",
      "----\n",
      "Phil Spector\n",
      "Phil Spector\n",
      "----\n",
      "Organizations could come together to address global issues\n",
      "The World Summit of Nobel Peace Laureates\n",
      "----\n",
      "yes\n",
      "yes\n",
      "----\n",
      "English Electric Canberra\n",
      "English Electric Canberra\n",
      "----\n",
      "2009 Big 12 Conference\n",
      "2009, Big 12 Conference\n",
      "----\n",
      "1,462\n",
      "1,462\n",
      "----\n",
      "Indianapolis Motor Speedway\n",
      "Indianapolis Motor Speedway\n",
      "----\n",
      "Rome\n",
      "New York City\n",
      "----\n",
      "Max Martin, Savan Kotecha and Ilya Salmanzadeh\n",
      "Max Martin\n",
      "----\n",
      "Marion, South Australia\n",
      "Marion\n",
      "----\n",
      "Drifting\n",
      "drifting\n",
      "----\n",
      "Keith Bostic\n",
      "Jerry Michael Glanville\n",
      "----\n",
      "35,124\n",
      "66,900\n",
      "----\n",
      "no\n",
      "\n",
      "----\n",
      "shortest player ever to play in the National Basketball Association\n",
      "head coach of the now-defunct WNBA team Charlotte Sting\n",
      "----\n",
      "Ronald Shusett\n",
      "Gordon Carroll, David Giler and Walter Hill\n",
      "----\n",
      "Adeline Virginia Woolf\n",
      "Virginia Woolf\n",
      "----\n",
      "821\n",
      "821\n",
      "----\n",
      "more than 70 countries\n",
      "more than 70 countries\n",
      "----\n",
      "Charmed\n",
      "Charmed\n",
      "----\n",
      "International Boxing Hall of Fame\n",
      "International Boxing Hall of Fame\n",
      "----\n",
      "Usher\n",
      "Usher\n",
      "----\n",
      "Bill Murray\n",
      "Bill Murray\n",
      "----\n",
      "Carabao Cup\n",
      "Carabao Cup\n",
      "----\n",
      "Teen Titans Go!\n",
      "Tara Strong\n",
      "----\n",
      "276,170 inhabitants\n",
      "inhabitants\n",
      "----\n",
      "orange\n",
      "orange clothing\n",
      "----\n",
      "Tromeo and Juliet\n",
      "Tromeo and Juliet\n",
      "----\n",
      "William Jefferson Clinton\n",
      "William Jefferson Clinton\n",
      "----\n",
      "John John Florence\n",
      "John John Florence\n",
      "----\n",
      "Ann\n",
      "Ann\n",
      "----\n",
      "Apalachees\n",
      "Ais native population\n",
      "----\n",
      "British\n",
      "British\n",
      "----\n",
      "1865\n",
      "1865\n",
      "----\n",
      "Newport\n",
      "Newport\n",
      "----\n",
      "Bob Seger\n",
      "Bob Seger\n",
      "----\n",
      "Conscription\n",
      "the draft\n",
      "----\n",
      "Mondelez International, Inc.\n",
      "Mondelez International\n",
      "----\n",
      "Monica Lewinsky\n",
      "Monica Lewinsky\n",
      "----\n",
      "April 1, 1949\n",
      "April 1, 1949\n",
      "----\n",
      "1866\n",
      "1866\n",
      "----\n",
      "Canary Islands, Spain\n",
      "Canary Islands, Spain\n",
      "----\n",
      "250 million\n",
      "more than 250 million\n",
      "----\n",
      "director\n",
      "director\n",
      "----\n",
      "The Conversation\n",
      "The Conversation\n",
      "----\n",
      "John Waters\n",
      "John Waters\n",
      "----\n",
      "Las Vegas Strip in Paradise\n",
      "Las Vegas, Nevada\n",
      "----\n",
      "no\n",
      "no\n",
      "----\n",
      "March and April\n",
      "March and April\n",
      "----\n",
      "Fairfax County\n",
      "Fairfax County\n",
      "----\n",
      "IT products and services\n",
      "IT products\n",
      "----\n",
      "Levni Yilmaz\n",
      "Lev Yilmaz\n",
      "----\n",
      "Beijing\n",
      "Beijing\n",
      "----\n",
      "no\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "result_f1 = []\n",
    "result_em = []\n",
    "for dev in dev_data:\n",
    "    predict = \"\"\n",
    "    answer = dev[\"answer\"].split(\"**Summary:\")[0].replace(\"**Answer:\", \"\").replace(\"<|im_start|>assistant\", \"\").strip()\n",
    "    generated_text = dev[\"generated_text\"].split(\"**Summary:\")[0].replace(\"**Answer:\", \"\")\n",
    "    if answer == \"yes\":\n",
    "        if answer in generated_text.lower() and \"no\" not in generated_text.lower():\n",
    "            generated_text = \"yes\"\n",
    "        else:\n",
    "            generated_text = \"\"\n",
    "    elif answer == \"no\":\n",
    "        if answer in generated_text.lower() and \"yes\" not in generated_text.lower():\n",
    "            generated_text = \"no\"\n",
    "        else:\n",
    "            generated_text = \"\"\n",
    "    answer = answer.strip()\n",
    "    predict = generated_text.strip()\n",
    "    print(answer)\n",
    "    print(predict)\n",
    "    print(\"----\")\n",
    "    result_f1.append(f1_score_hotpot(answer, predict))\n",
    "    result_em.append(exact_match_score(predict, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 점수:  0.684968253968254\n",
      "EM 점수:  0.56\n"
     ]
    }
   ],
   "source": [
    "# F1 점수와 EM 점수 출력\n",
    "print(\"F1 점수: \", sum(result_f1) / len(result_f1))\n",
    "print(\"EM 점수: \", sum(result_em) / len(result_em))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'qwen_answer_cnn_50.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen_answer_cnn_50.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     dev_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/workspace/XAI_rationale-inference-LLM/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qwen_answer_cnn_50.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"qwen_answer_cnn_50.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dev_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(predicted_summary, reference_summary):\n",
    "    # ROUGE 계산기 생성 (rouge1, rouge2, rougeL을 사용)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    scores = scorer.score(reference_summary, predicted_summary)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June . Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "Palestinians joined the International Criminal Court on Wednesday, becoming the 123rd member. The move gives the court jurisdiction over alleged crimes in Palestinian territories.\n",
      "----\n",
      "Theia, a bully breed mix, was apparently hit by a car, whacked with a hammer and buried in a field . \"She's a true miracle dog and she deserves a good life,\" says Sara Mellado, who is looking for a home for Theia .\n",
      "Theia, a dog who was hit by a car, beaten with a hammer and buried. Theia survived. A dog who was hit by a car, beaten with a hammer and buried, has survived. She's now at a veterinary teaching hospital.\n",
      "----\n",
      "Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister . He once participated in a takeover of the Iranian Consulate in San Francisco . The Iranian foreign minister tweets in English .\n",
      "Mohammad Javad Zarif is the Iranian foreign minister. He was nominated to be foreign minister by Hassan Rouhami. He received a hero's welcome as he arrived in Iran on a sunny Friday morning.\n",
      "----\n",
      "17 Americans were exposed to the Ebola virus while in Sierra Leone in March . Another person was diagnosed with the disease and taken to hospital in Maryland . National Institutes of Health says the patient is in fair condition after weeks of treatment .\n",
      "CDC says last of 17 patients will be released by Thursday. Five Americans who were monitored at a Nebraska hospital have been released.\n",
      "----\n",
      "Student is no longer on Duke University campus and will face disciplinary review . School officials identified student during investigation and the person admitted to hanging the noose, Duke says . The noose, made of rope, was discovered on campus about 2 a.m.\n",
      "Duke University student admitted to hanging noose. No other people were involved, but criminal investigations are ongoing. Duke University students march after discovery.\n",
      "----\n",
      "College-bound basketball star asks girl with Down syndrome to high school prom . Pictures of the two during the \"prom-posal\" have gone viral .\n",
      "Trey Moses, a star basketball player at Eastern High School in Louisville, Kentucky, asked Ellie Meredith, a 15-year-old with Down syndrome, to prom. The pair met through a special program at their school that pairs students with special needs with others who don't. Trey's mother Shelly Moses says the pair have a lot in common: \"They both love Taylor Swift and they both love to play basketball.\"\n",
      "----\n",
      "Amnesty's annual death penalty report catalogs encouraging signs, but setbacks in numbers of those sentenced to death . Organization claims that governments around the world are using the threat of terrorism to advance executions . The number of executions worldwide has gone down by almost 22% compared with 2013, but death sentences up by 28% .\n",
      "Amnesty International's annual report on the death penalty shows that the number of executions worldwide has gone down by almost 22% on the previous year. At least 607 people were executed around the world in 2014, compared to 778 in 2013. The report cites the example of Pakistan lifting a six-year moratorium on the execution of civilians following the horrific attack on a school in Peshawar in December. The report notes that the spike in sentencing is attributable to mass-sentencing in countries including Egypt and Nigeria, \"against scores of people in some cases.\" Opinion: Sharp spike in death sentences.\n",
      "----\n",
      "Andrew Getty's death appears to be from natural causes, police say, citing coroner's early assessment . In a petition for a restraining order, Getty had written he had a serious medical condition. Police say this is not a criminal matter at this time .\n",
      "Andrew Getty, 47, appears to have died of natural causes, a Los Angeles Police Department spokesman said. He had several health issues, an autopsy will be conducted.\n",
      "----\n",
      "Once a super typhoon, Maysak is now a tropical storm with 70 mph winds . It could still cause flooding, landslides and other problems in the Philippines .\n",
      "The storm is classified as a tropical storm, but could still cause flooding and landslides. It's forecast to make landfall Sunday morning on the southeastern coast of Isabela province.\n",
      "----\n",
      "Bob Barker returned to host \"The Price Is Right\" on Wednesday . Barker, 91, had retired as host in 2007 .\n",
      "Bob Barker, 91, hosted his first \"Price is Right\" show since 2007. He will host the show's finale May 29.\n",
      "----\n",
      "London's Metropolitan Police say the man was arrested at Luton airport after landing on a flight from Istanbul . He's been charged with terror offenses allegedly committed since the start of November .\n",
      "The 19-year-old has been charged with terror offenses after being arrested at Luton Airport. The teenager, who is a UK national, had traveled from Istanbul on a flight.\n",
      "----\n",
      "\"Furious 7\" pays tribute to star Paul Walker, who died during filming . Vin Diesel: \"This movie is more than a movie\" \"Furious 7\" opens Friday .\n",
      "Paul Walker died in a car crash in 2013. He was the star of the \"Fast & Furious\" franchise. His brother Cody says he would be proud of \"Furious 7\".\n",
      "----\n",
      "Museum: Anne Frank died earlier than previously believed . Researchers re-examined archives and testimonies of survivors . Anne and older sister Margot Frank are believed to have died in February 1945 .\n",
      "New research suggests Anne Frank died a month earlier than previously thought. New study re-examined archives of the Red Cross, the International Training Service and the Bergen-Belsen Memorial. Researchers concluded that Anne and her older sister, Margot Frank, died at least a month earlier than previously thought.\n",
      "----\n",
      "LZ: Indiana law pushing back LGBT rights, and other states' anti-LGBT moves, bow to far right wing that GOP candidates need for 2016 . Cruz, Huckabee, Jindal, Carson, Walker are reviving culture wars, he says.  Equality for LGBT has not yet \"won\" in America .\n",
      "Mike Pence, a favorite of the Koch brothers, signed a religious freedom bill that allows discrimination against gays and lesbians. The move drew criticism from the GOP and the Democratic Party. Republicans are now trying to figure out how to handle the backlash. The move may help them in the primaries but could hurt them in the general election.\n",
      "----\n",
      "Singing the national anthem is a risky proposition . Whitney Houston nailed it; Roseanne Barr destroyed it .\n",
      "Vince Neil: \"The Star-Spangled Banner\" singer mangled the national anthem at a Las Vegas football game. He was booed by the crowd. Jimi Hendrix: Guitarist's psychedelic version of the anthem inflamed mainstream America. Whitney Houston: Singer's rendition at Super Bowl XXV set the modern standard. Roseanne Barr: Comedian's rendition of the anthem at a San Diego Padres game was booed. Michael Bolton: Singer's rendition of the anthem at an American League Championship Series game was overwrought.\n",
      "----\n",
      "While Republican Gov. Asa Hutchinson was weighing an Arkansas religious freedom bill, Walmart voiced its opposition . Walmart and other high-profile businesses are showing their support for gay and lesbian rights . Their stance puts them in conflict with socially conservative Republicans, traditionally seen as allies .\n",
      "Walmart's opposition to a religious freedom law in Arkansas is part of a broader trend. The company's opposition to the measure comes as it boosts wages and expands health benefits.\n",
      "----\n",
      "Amnesty International releases its annual review of the death penalty worldwide; much of it makes for grim reading . Salil Shetty: Countries that use executions to deal with problems are on the wrong side of history .\n",
      "The death penalty was used to tackle crime and terrorism in 2014, according to a new report. Amnesty International says many of those executed were drug traffickers. In Pakistan, the government lifted a six-year moratorium on the death penalty. In Iran, the government said it was a move to end a surge in violent crime. The death penalty was used to tackle crime and terrorism in 2014, according to a new report. Amnesty International says many of those executed were drug traffickers. In Pakistan, the government lifted a six-year moratorium on the death penalty. In Iran, the government said it was a move to end a surge in violent crime. The death penalty was used to tackle crime and terrorism in 2014, according to a new report. Amnesty International says many of those executed were drug traffickers. In Pakistan, the government lifted a six-year moratorium on the death penalty. In Iran, the government said it was a move to end a surge in violent crime. The death penalty was used to tackle crime and terrorism in 2014, according to a new report. Amnesty International says many of those executed were drug traffickers. In Pakistan, the government\n",
      "----\n",
      "Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports . Journalists at Bild and Paris Match are \"very confident\" the video clip is real, an editor says . Andreas Lubitz had informed his Lufthansa training school of an episode of severe depression, airline says .\n",
      "French prosecutor says no video footage was recovered from Germanwings Flight 9525 crash site. Paris Match and Bild claim they watched cell phone video showing final moments of plane. No cell phones have been sent to criminal research institute for analysis. Two sources say Lubitz was in good health when he flew Germanwings Flight 9525.\n",
      "----\n",
      "The Rev. Robert Schuller, 88, had been diagnosed with esophageal cancer in 2013 . His TV show, \"Hour of Power,\" was enormously popular in the 1970s and 1980s .\n",
      "Robert H. Schuller, 88, televangelist and founder of \"Hour of Power\" died Thursday. Schuller was born in an Iowa farmhouse without running water. He and his wife met while she played the organ at his church.\n",
      "----\n",
      "Former GOP representative compares President Obama to Andreas Lubitz . Bachmann said with possible Iran deal, Obama will fly \"entire nation into the rocks\" Reaction on social media? She was blasted by Facebook commenters .\n",
      "Bachmann compares Obama to Andreas Lubitz, the co-pilot of Germanwings Flight 9525. The congresswoman says Obama is \"for the 300 million souls of the United States\" what Lubitz was for 150 souls.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "rougeL = []\n",
    "for dev in dev_data:\n",
    "    predict = \"\"\n",
    "    answer = dev[\"answer\"].split(\"**Summary**\")[1].strip()\n",
    "    generated_text = dev[\"generated_text\"].split(\"assistant\\n\")[1]\n",
    "    if \"**Answer**\" in generated_text:\n",
    "        predict = generated_text.split(\"**Answer**\")[1].replace(\"**Summary**\\n\", \"\")\n",
    "    else:\n",
    "        predict = generated_text\n",
    "    \n",
    "    answer = answer.strip()\n",
    "    predict = predict.strip()\n",
    "    print(answer)\n",
    "    print(predict)\n",
    "    print(\"----\")\n",
    "    rouge_scores = calculate_rouge(predict, answer)\n",
    "    rouge1.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL.append(rouge_scores['rougeL'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33227131701560736\n",
      "0.13398218246023427\n",
      "0.2444843813224157\n"
     ]
    }
   ],
   "source": [
    "print(sum(rouge1)/len(rouge1))\n",
    "print(sum(rouge2)/len(rouge2))\n",
    "print(sum(rougeL)/len(rougeL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 근거 문장 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result/1103+dataup+loss/hotpot_tt_8000.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/1029data/hotpot_dev_supporting.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    dev_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_precision_recall(predicted, ground_truth):\n",
    "    # Exact Match\n",
    "    em = int(predicted == ground_truth)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    predicted_set = set(predicted)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    \n",
    "    true_positives = len(predicted_set & ground_truth_set)\n",
    "    \n",
    "    precision = true_positives / len(predicted_set) if predicted_set else 0\n",
    "    recall = true_positives / len(ground_truth_set) if ground_truth_set else 0\n",
    "    \n",
    "    return em, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greenwich Village, New York City\n",
      "Greenwich Village\n",
      "\n",
      "Adriana Trigiani is an Italian American best-selling author of sixteen books, television writer, film director, and entrepreneur based in Greenwich Village, New York City.\n",
      "================\n",
      "David Weissman\n",
      "David Weissman\n",
      "\n",
      "The Family Man is a 2000 American romantic comedy-drama film directed by Brett Ratner, written by David Diamond and David Weissman, and starring Nicolas Cage and Téa Leoni.\n",
      "================\n",
      "Robert Erskine Childers DSC\n",
      "Robert Erskine Childers\n",
      "\n",
      "His double first cousin and close friend was Robert Erskine Childers.\n",
      "================\n",
      "Fujioka, Gunma\n",
      "Japan\n",
      "\n",
      "Kyo (京 , Kyō ) is a Japanese musician, poet and singer-songwriter.\n",
      "================\n",
      "Letters to Cleo\n",
      "Screaming Trees\n",
      "\n",
      "Beat Happening/Screaming Trees is an EP and a one-off collaboration between Beat Happening (from Olympia, Washington) and Screaming Trees (from Ellensburg, Washington).\n",
      "================\n",
      "New York City\n",
      "New York City\n",
      "\n",
      "Columbia University (Columbia; officially Columbia University in the City of New York), established in 1754, is a private Ivy League research university in Upper Manhattan, New York City, often cited as one of the world's most prestigious universities.\n",
      "================\n",
      "Yellowcraig\n",
      "Firth of Forth\n",
      "\n",
      "Yellowcraig is partly within the Firth of Forth Site of Special Scientific Interest (SSSI).\n",
      "================\n",
      "Adeline Virginia Woolf\n",
      "Virginia Woolf\n",
      "\n",
      "\"The Duchess and the Jeweller\" (1938) is a short story by Virginia Woolf.\n",
      "================\n",
      "Charmed\n",
      "Charmed\n",
      "\n",
      "The series narrative follows a trio of sisters, known as The Charmed Ones, the most powerful good witches of all time, who use their combined \"Power of Three\" to protect innocent lives from evil beings such as demons and warlocks.\n",
      "================\n",
      "Teen Titans Go!\n",
      "Tara Strong\n",
      "\n",
      "It stars Scott Menville, Hynden Walch, Khary Payton, Tara Strong, and Greg Cipes as the voices of the main characters.\n",
      "================\n",
      "Tromeo and Juliet\n",
      "Tromeo and Juliet\n",
      "\n",
      "Tromeo and Juliet is a 1996 American independent transgressive romantic comedy film and a loose adaptation of William Shakespeare's \"Romeo & Juliet\" from Troma Entertainment.\n",
      "================\n",
      "Bob Seger\n",
      "Bob Seger\n",
      "\n",
      "Against the Wind is the eleventh album by American rock singer Bob Seger and his fourth with the Silver Bullet Band.\n",
      "================\n",
      "Mondelez International, Inc.\n",
      "Mondelez International\n",
      "\n",
      "Mondelez International, Inc., styled Mondelēz ( ), is an American multinational confectionery, food, and beverage company based in Illinois which employs about 107,000 people around the world.\n",
      "================\n",
      "Monica Lewinsky\n",
      "Monica Lewinsky\n",
      "\n",
      "William H. Ginsburg (March 25, 1943 – April 1, 2013) was an American lawyer, best known for representing former White House intern Monica Lewinsky in her controversy regarding sexual activities with President Bill Clinton in 1998.\n",
      "================\n",
      "director\n",
      "director\n",
      "\n",
      "Aram A. Avakian (April 23, 1926 – January 17, 1987) was an Armenian-American film editor and director.\n",
      "================\n",
      "Fairfax County\n",
      "Fairfax County\n",
      "\n",
      "McLean ( ) is a census-designated place (CDP) in Fairfax County in Northern Virginia.\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "all_em = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "ignore = 0\n",
    "for dev, data in zip(dev_data, test_data):\n",
    "    assert dev[\"_id\"] == data[\"_id\"]\n",
    "    predict = \"\"\n",
    "    answer = data[\"answer\"].split(\"**Summary:\")[0].replace(\"**Answer:\", \"\").replace(\"<|im_start|>assistant\", \"\").strip()\n",
    "    generated_text = data[\"generated_text\"].split(\"**Summary:\")[0].replace(\"**Answer:\", \"\")\n",
    "    if answer == \"yes\":\n",
    "        if answer in generated_text.lower() and \"no\" not in generated_text.lower():\n",
    "            generated_text = \"yes\"\n",
    "        else:\n",
    "            generated_text = \"\"\n",
    "    elif answer == \"no\":\n",
    "        if answer in generated_text.lower() and \"yes\" not in generated_text.lower():\n",
    "            generated_text = \"no\"\n",
    "        else:\n",
    "            generated_text = \"\"\n",
    "    answer = answer.strip()\n",
    "    predict = generated_text.strip()\n",
    "    result_f1.append(f1_score_hotpot(answer, predict))\n",
    "    result_em.append(exact_match_score(predict, answer))\n",
    "    ################################################\n",
    "    gold_sp = data[\"gold_sp\"]\n",
    "    pred_sp = data[\"pred_sp\"]\n",
    "    em, precision, recall = em_precision_recall(pred_sp, gold_sp)\n",
    "    all_em.append(em)\n",
    "    all_precision.append(precision)\n",
    "    all_recall.append(recall)\n",
    "    # print(f\"EM: {em}, Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "    \n",
    "    for i in pred_sp:\n",
    "        if answer == \"yes\" or answer == \"no\":\n",
    "            ignore += 1\n",
    "            break\n",
    "        if predict in dev[\"sent\"][i-1]:\n",
    "            score.append(dev[\"_id\"])\n",
    "            print(answer)\n",
    "            print(generated_text)\n",
    "            print(dev[\"sent\"][i-1])\n",
    "            print(\"================\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 점수:  0.684968253968254\n",
      "EM 점수:  0.56\n"
     ]
    }
   ],
   "source": [
    "# F1 점수와 EM 점수 출력\n",
    "print(\"F1 점수: \", sum(result_f1) / len(result_f1))\n",
    "print(\"EM 점수: \", sum(result_em) / len(result_em))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_em 점수:  0.0\n",
      "all_precision 점수:  0.04999999999999999\n",
      "all_recall 점수:  0.06083333333333333\n"
     ]
    }
   ],
   "source": [
    "# F1 점수와 EM 점수 출력\n",
    "print(\"all_em 점수: \", sum(all_em) / len(all_em))\n",
    "print(\"all_precision 점수: \", sum(all_precision) / len(all_precision))\n",
    "print(\"all_recall 점수: \", sum(all_recall) / len(all_recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
